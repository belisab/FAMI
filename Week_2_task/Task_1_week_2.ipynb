{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1f45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2925381",
   "metadata": {},
   "source": [
    "After downloading the data from https://moodle.helsinki.fi/pluginfile.php/6645850/mod_page/content/19/enwiki-20181001-corpus.100-articles.txt, I split the individual articles using BeautifulSoup by selecting the <article> tags. Then I converted the bs4 elements to strings and added them to the documents list I had created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97057afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles in the document is: 100\n",
      "The number of documents in the list is: 100 Type of documents: <class 'str'>\n",
      "First 500 characters of the first document:\n",
      " <article name=\"Anarchism\">\n",
      "Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical or free associations. Anarchism holds the state to be undesirable, unnecessary and harmful. According to Peter Kropotkin, Godwin was \"the first to formulate the political and economical conceptions of anarchism, \n"
     ]
    }
   ],
   "source": [
    "# use this code only if you want to use the enwiki-20181001-corpus.100-articles.txt file\n",
    "\n",
    "with open (\"enwiki-20181001-corpus.100-articles.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "soup = BeautifulSoup(document, 'html.parser')\n",
    "articles = soup.find_all('article')\n",
    "print(f'The number of articles in the document is: {len(articles)}')\n",
    "\n",
    "documents = []\n",
    "for article in articles:\n",
    "    documents.append(str(article))\n",
    "\n",
    "print(f'The number of documents in the list is: {len(documents)}', \"Type of documents:\", type(documents[0]))\n",
    "print(\"First 500 characters of the first document:\\n\", documents[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d76e2",
   "metadata": {},
   "source": [
    "Now it's time to adapt the code for html we scraped from wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c1eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in the list is: 1225 Type of documents: <class 'str'>\n",
      "First 500 characters of the first document:\n",
      " Hello fellow Wikipedians,\n",
      "\n",
      "I have just modified one external link on 10th edition of Systema Naturae . Please take a moment to review my edit . If you have any questions, or need the bot to ignore the links, or the page altogether, please visit this simple FaQ for additional information. I made the following changes:\n",
      "\n",
      "When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.\n",
      "\n",
      "This message was posted before February 2018. A\n"
     ]
    }
   ],
   "source": [
    "# Processing multiple HTML files in a folder\n",
    "folder_path = r\"wikipedia_talk_pages\"\n",
    "documents = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".html\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "            result = []\n",
    "\n",
    "            # page title \n",
    "            title = soup.find(\"span\", class_=\"mw-page-title-main\")\n",
    "            if title:\n",
    "                result.append(title.get_text(strip=True)) # strip removes leading/trailing whitespace\n",
    "\n",
    "            # main content\n",
    "            root = soup.find(\"div\", id=\"mw-content-text\") # we create root to be sure we are in the content section\n",
    "            content = root.find(\"div\", class_=\"mw-parser-output\") if root else None # it verifies root is not None\n",
    "            # in content, we look for h2, h3, and p tags that are direct children (not nested deeper)\n",
    "\n",
    "            if content:\n",
    "                for el in content.find_all([\"h2\", \"h3\", \"p\"], recursive=False): # recursive=False ensures we only get direct children\n",
    "                    text = el.get_text(\" \", strip=True)\n",
    "                    if text:\n",
    "                        result.append(text)\n",
    "\n",
    "            full_text = \"\\n\\n\".join(result) # every wiki page is stored as a single string with double newlines between sections\n",
    "            if full_text:   # only add non-empty documents    \n",
    "                documents.append(full_text)\n",
    "\n",
    "print(f'The number of documents in the list is: {len(documents)}', \"Type of documents:\", type(documents[0]))\n",
    "print(\"First 500 characters of the first document:\\n\", documents[0][:500])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db03d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operators and/AND, or/OR, not/NOT become &, |, 1 -\n",
    "# Parentheses are left untouched\n",
    "# Everything else is interpreted as a term and fed\n",
    "# through td_matrix[t2i[\"...\"]]\n",
    "d = {\"and\": \"&\", \"AND\": \"&\",\n",
    "         \"or\": \"|\", \"OR\": \"|\",\n",
    "         \"not\": \"1 -\", \"NOT\": \"1 -\",\n",
    "         \"(\": \"(\", \")\": \")\"}  # operator replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0878f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(*args):\n",
    "    # If you want debug prints to show up, uncomment the line below: \n",
    "    # print(*args)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d963ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite tokens\n",
    "def rewrite_token(t):\n",
    "    # If the search term exists in our dictionary of operators, get it, \n",
    "    # otherwise find occurrences of the term in `td_matrix``. If the term \n",
    "    # is not in our dictionary, then the query results in 0 (since the \n",
    "    # term does not occur in any of the documents)\n",
    "    return d.get(t, f'(td_matrix[t2i[\"{t}\"]] if \"{t}\" in t2i else empty_row)')\n",
    "\n",
    "def rewrite_query(query):\n",
    "    # Rewrite every token in the query\n",
    "    return \" \".join(rewrite_token(t) for t in query.split())\n",
    "\n",
    "def test_query(query, td_matrix, t2i, documents):\n",
    "    # Generate a row of all zeroes for queries containing words not in our \n",
    "    # dictionary\n",
    "    empty_row = np.matrix(np.repeat(0, td_matrix.shape[1]))\n",
    "\n",
    "    rewritten = rewrite_query(query)\n",
    "    debug_print(\"Query: '\" + query + \"'\")\n",
    "    debug_print(\"Rewritten:\", rewritten)\n",
    "\n",
    "    # Eval runs the string as a Python command\n",
    "    # `td_matrix`, `t2i`, and `empty_row` have to be in scope in \n",
    "    # order for eval() to work\n",
    "    eval_result = eval(rewritten)\n",
    "    debug_print(\"Matching:\", eval_result)\n",
    "\n",
    "    # Finding the matching document\n",
    "    hits_matrix = eval_result\n",
    "    hits_list = list(hits_matrix.nonzero()[1])\n",
    "\n",
    "    print(f\"Found {len(hits_list)} results\")\n",
    "\n",
    "    # Prints the first 500 characters of the matching document\n",
    "    for i, doc_idx in enumerate(hits_list):\n",
    "        print(f\"Matching doc #{i}: {documents[doc_idx][:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e913415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat and witch\n",
      "Found 2 results\n",
      "Matching doc #0: Black cat\n",
      "\n",
      "\" Another possible theory as to how the plague spread so quickly is that by killing many of the cats (believed to be witches' familiars) during the witch hunts caused the rodent population to rise, and with them rose the probability of infection .\"  Taken from an anon, 68.174.249.133, contribution from the Black Death , later reverted.\n",
      "\n",
      "I've heard this theory, but don't have the materials on hand to substantiate it.  Actually it was probably a social result of the ongoing waves of pla\n",
      "Matching doc #1: Cultural depictions of cats\n",
      "\n",
      "A number of suggestions\n",
      "\n",
      "-- Davelane 19:21, 26 December 2005 (UTC) [ reply ]\n",
      "\n",
      "\"The human killing of cats in the Middle Ages has also been cited as one of the reasons for the spread of bubonic plague - the Black Death, which was spread by the increased rodent population caused by the death of so many cats.\"\n",
      "\n",
      "This passage is internally inconsistent with Wikipedia's own account of the Black Death.  It states that the disease was equally virulent across Europe, the Middl\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    cv = CountVectorizer(lowercase=True, binary=True)\n",
    "\n",
    "    sparse_matrix = cv.fit_transform(documents)\n",
    "    debug_print(\"Term-document matrix: (?)\\n\")\n",
    "    debug_print(sparse_matrix)\n",
    "\n",
    "    dense_matrix = sparse_matrix.todense()\n",
    "    debug_print(\"Term-document matrix: (?)\\n\")\n",
    "    debug_print(dense_matrix)\n",
    "\n",
    "    td_matrix = dense_matrix.T  # .T transposes the matrix\n",
    "    debug_print(\"Term-document matrix:\\n\")\n",
    "    debug_print(td_matrix)\n",
    "\n",
    "    t2i = cv.vocabulary_\n",
    "    debug_print(t2i)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Search for something. If you want to stop your search \"\n",
    "                      \"type 'q'. Search: \")\n",
    "        \n",
    "        # Remove any leading & trailing whitespace from the query and turn it to \n",
    "        # lowercase for case insensitive searching\n",
    "        query = query.lower().strip()\n",
    "\n",
    "        # Check for empty queries and continue asking for input if so\n",
    "        if query == \"\":\n",
    "            continue\n",
    "\n",
    "        if query == \"q\":\n",
    "            break\n",
    "        else:\n",
    "            print(query)\n",
    "            # because td_matrix and t2i are defined in main(), also pass these\n",
    "            # to other functions\n",
    "            test_query(query, td_matrix, t2i, documents)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
